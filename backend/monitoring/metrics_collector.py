"""
Metrics Collector - Prometheus-compatible monitoring
"""

from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry
import time
import psutil
import logging
from typing import Dict, Any, Optional
from contextlib import contextmanager

logger = logging.getLogger(__name__)

class MetricsCollector:
    """
    Collects and exposes metrics for monitoring Frankenstino AI
    """

    def __init__(self, registry: Optional[CollectorRegistry] = None):
        self.registry = registry or CollectorRegistry()

        # Request metrics
        self.requests_total = Counter(
            'frankenstino_requests_total',
            'Total number of requests',
            ['endpoint', 'method', 'status'],
            registry=self.registry
        )

        self.request_duration = Histogram(
            'frankenstino_request_duration_seconds',
            'Request duration in seconds',
            ['endpoint', 'method'],
            registry=self.registry
        )

        # Memory metrics
        self.memory_chunks_total = Gauge(
            'frankenstino_memory_chunks_total',
            'Total number of memory chunks',
            registry=self.registry
        )

        self.memory_vector_store_size = Gauge(
            'frankenstino_memory_vector_store_size_mb',
            'Vector store size in MB',
            registry=self.registry
        )

        # LLM metrics
        self.llm_tokens_generated = Counter(
            'frankenstino_llm_tokens_generated_total',
            'Total tokens generated by LLM',
            ['model'],
            registry=self.registry
        )

        self.llm_inference_time = Histogram(
            'frankenstino_llm_inference_time_seconds',
            'LLM inference time in seconds',
            ['model', 'task'],
            registry=self.registry
        )

        # Memory processor metrics
        self.lightweight_processor_usage = Counter(
            'frankenstino_lightweight_processor_usage_total',
            'Usage count of lightweight processor',
            ['operation'],
            registry=self.registry
        )

        # Error metrics
        self.errors_total = Counter(
            'frankenstino_errors_total',
            'Total number of errors',
            ['type', 'component'],
            registry=self.registry
        )

        # System metrics
        self.cpu_usage = Gauge(
            'frankenstino_cpu_usage_percent',
            'CPU usage percentage',
            registry=self.registry
        )

        self.memory_usage = Gauge(
            'frankenstino_memory_usage_mb',
            'Memory usage in MB',
            registry=self.registry
        )

        # Custom metrics for AGI.MD requirements
        self.hallucination_score = Histogram(
            'frankenstino_hallucination_score',
            'Hallucination detection score (0-1)',
            registry=self.registry
        )

        self.retrieval_precision = Histogram(
            'frankenstino_retrieval_precision_at_k',
            'Retrieval precision@K score',
            ['k'],
            registry=self.registry
        )

    @contextmanager
    def measure_request(self, endpoint: str, method: str):
        """Context manager to measure request duration"""
        start_time = time.time()
        try:
            yield
            status = 'success'
        except Exception as e:
            status = 'error'
            self.errors_total.labels(type='request', component=endpoint).inc()
            raise
        finally:
            duration = time.time() - start_time
            self.requests_total.labels(
                endpoint=endpoint, method=method, status=status
            ).inc()
            self.request_duration.labels(
                endpoint=endpoint, method=method
            ).observe(duration)

    def record_memory_stats(self, memory_manager):
        """Record memory-related statistics"""
        try:
            # Update memory chunk count
            total_chunks = len(memory_manager.list_memory_chunks())
            self.memory_chunks_total.set(total_chunks)

            # Update vector store size
            if hasattr(memory_manager, 'vector_store'):
                stats = memory_manager.vector_store.get_stats()
                self.memory_vector_store_size.set(stats.get('index_size_mb', 0))

        except Exception as e:
            logger.error(f"Failed to record memory stats: {e}")

    def record_llm_usage(self, model_name: str, tokens: int, task: str, duration: float):
        """Record LLM usage metrics"""
        self.llm_tokens_generated.labels(model=model_name).inc(tokens)
        self.llm_inference_time.labels(model=model_name, task=task).observe(duration)

    def record_lightweight_processor_usage(self, operation: str):
        """Record lightweight processor usage"""
        self.lightweight_processor_usage.labels(operation=operation).inc()

    def record_system_stats(self):
        """Record system resource usage"""
        try:
            self.cpu_usage.set(psutil.cpu_percent(interval=1))
            self.memory_usage.set(psutil.virtual_memory().used / (1024 * 1024))
        except Exception as e:
            logger.error(f"Failed to record system stats: {e}")

    def record_hallucination_score(self, score: float):
        """Record hallucination detection score"""
        self.hallucination_score.observe(score)

    def record_retrieval_precision(self, k: int, precision: float):
        """Record retrieval precision@K"""
        self.retrieval_precision.labels(k=str(k)).observe(precision)

    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics snapshot"""
        return {
            'memory_chunks': self.memory_chunks_total._value,
            'vector_store_size_mb': self.memory_vector_store_size._value,
            'cpu_usage_percent': self.cpu_usage._value,
            'memory_usage_mb': self.memory_usage._value,
            'errors_total': sum(self.errors_total._samples()),
            'requests_total': sum(self.requests_total._samples())
        }
